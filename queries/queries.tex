\chapter{Query Processing}
\label{chap:Query Processing}
So far, we have studied base techniques that improve OLAP performance for in-memory, read-only databases. However, a high-performance \bd~application does not only rely on using the correct data format, compression, indexes, parallelization, and implementation. In this chapter we elaborate query processing techniques.

In Section \ref{sub:Business Discovery, Queries, and SQL}, we explained how \bd~applications relate to traditional queries. We saw that a \bd~application must support listing of rows, filtering, joining, and grouping/aggregation. In this chapter, we focus on query processing, mainly joining, grouping and aggregation, and query optimization.


%storing the data in the correct format, using light-weight compression algorithms, and index and other structures to aid performance. We show that algorithms and optimizations done when processing queries are also important.

%Grouping and Aggregation are the single most important functionality for a \bi~applications, as people require data summary and not data details. Joining is key for these applications, and perhaps even more for \bd~systems, since a filter in one table must quickly propagate to the other tables.

%As mentioned in the background section lthough a \bd~system does not need to support SQL, we still need traditional query techniques, like joining, grouping and aggregation.

\newpage

\section{Joining}
\label{sec:Joining}
Joining is a common database operation that combines records from two or more tables. In our research, we have studied several DBMSes, and seen how their join algorithms work. As we saw in Section \ref{sub:Business Discovery, Queries, and SQL}, joining is also needed for a \bd~application such that interaction with one table can affect the selection, filters, or aggregations in other tables.

For joining two tables, three main methods exist \cite{Bratbergsengen2015-ed}: 
\begin{itemize}
  \item \textit{Partition-based approach}, where records of both relations are split into groups based on the hash value of the keys. This technique is most effective on data volumes are too large to fit in main memory.
  \item \textit{Sort-merge approach}, where both tables are sorted and then merge results by concatenating records with equal key values. This approach is most effective when one or both operands are sorted in advance.
  \item \textit{Nested loop approach}, which compares all rows in both tables.
\end{itemize}

We find the \textit{nested loop} approach to be the most popular joining algorithm for in-memory and parallel databases \cite{Boncz2002-yj}. The rest of Section \ref{sec:Joining} will therefore only discuss this method.

\subsection{Nested Loop Algorithms}
\label{sub:Nested Loop Algorithms}

% Basic nested join functionality
In its simplest form, the \textit{nested loop} method compares all the join keys in all rows directly using a double loop. This simple algorithm has a runtime of $O(n*m)$ where $n$ and $m$ is the size of tables A and B respectively. However, to improve performance in a \textit{nested loop} algorithm, hashmaps are commonly used. Usually, the join is performed by hashing the smaller (inner) relation first, then probe the hashmap by scanning the larger (outer) relation. 

In a star or snowflake schema, dimension tables are usually considered as the inner relations \cite{Barber2012-xt, Raman2013-em}. In other words, the keys of these tables will be used to create the hashmaps in the join. In the second phase of the algorithm, the fact table will probe the generated hashmaps to complete the join. 

\ffigure{img/nested-loop.png}{An example nested loop join structure. Records are first tested against a Bloom filter. If found in the filter, the join key is searched in the join structure. Records are first hashed, and then each entry in the hash table is the root of a binary tree. Courtesy of \cite{Bratbergsengen2015-ed}.}{fig:nested-loop}

Bratbergsengen \ea~show how a combination of hash tables, bloom filters, and binary trees can be used in a \textit{nested join} algorithm. In the probe phase, keys are first checked towards a Bloom filter (Section \ref{sub:Bloom Filter}). Bloom filters never return false negatives and is an efficient way of reducing the numbers of keys entering the join. If the key is found in the Bloom filter lookup, it is hashed and checked up against a hybrid hashmap/binary-tree structure. In this structure, each entry in the hashmap is the root node of a binary tree which are used to efficiently look up values. The join algorithm is illustrated in Figure \ref{fig:nested-loop}.

% Modifications
In the nested loop approach, one of the operands might be partitioned \cite{Bratbergsengen2015-ed}. For example, a join might be partitioned by only hashing a subset of the inner relation at a time. The entire join algorithm will then include several probe passes over the outer relation, one for each subset. Historically, this technique has been applied to ensure the whole hash structure fits in RAM. We conclude that a similar concept can be applied to CPU caches and that the join algorithm might benefit from partitioning the inner relation such that each subset fits in the CPU cache.

We see that several DBMSes in this research use a \textit{nested loop} join variant with Bloom filters, including \oracle~\cite{Lahiri2015-mz}, \ibm~\cite{Raman2013-em} and \blink~\cite{Raman2008-gi}. Barber \ea~explains how Bloom filters are effective in eliminating non-matching join outers before they enter the join \cite{Barber2014-ey}.

Our research has shown that one of the key design goals for efficient joining using the \textit{nested loop} approach with hashmaps, is to keep the hash tables collision free \cite{Raman2008-gi, Raman2013-em}. One way to ensure this is to use the dictionary keys in \de~as a perfect hashing function. If a table is joined on several keys, a minimal perfect hash can be calculated.

Regarding implementation, it is important that the algorithm and the hash table are \textit{cache-aware}. One way to improve cache performance in a \textit{nested loop} join, is to use linear probing instead of open-chain addressing for the hashmap \cite{Raman2008-gi}. Open-chain addressing should only be used for overflow buckets.

Joining should be done in parallel for ultimate performance. For instance, the hash tables can be built using multiple threads \cite{Barber2014-ey}. We see that some \textit{nested loop} based algorithms partition the input to improve cache performance. However, this decision might as well be a parallelization decision \cite{Neumann2011-uq}. A system studied in our research that employs high-performance parallel joining is \ibm \cite{Raman2013-em}.

\subsection{Invisible Join}
\label{sub:Invisible Join}

\begin{figure}
  \centering
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{img/invisible-join-1.png}
    \caption{The first phase of the \textit{invisible join}. Predicates are evaluated on every dimension table, and matching rows are put into a hash table.}
    \label{fig:invisible-join-1} 
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{img/invisible-join-2.png}
    \caption{The second phase of the \textit{invisible join}. The fact table is scanned and probed towards the hash tables generated in in the previous step. Each relation generates a bitmap which is combined using a bitwise \texttt{AND} operation.}
    \label{fig:invisible-join-2} 
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{img/invisible-join-3.png}
    \caption{The third phase of the \textit{invisible join}. The foreign keys for the join columns are extracted from the fact table. The extracted values use the  hash tables from the firt phase to complete the join.}
    \label{fig:invisible-join-3} 
  \end{subfigure}
  \caption{The \textit{invisible join} algorithm from Abadi \ea~Courtesy of \cite{Abadi2008-dd}.}
  \label{fig:invisible-join} 
\end{figure}

Our research has identified several different join implementations that are suited for OLAP workloads and in-memory processing. In this section, we will focus on one such implementation, namely Abadi \ea's \textit{invisible join} \cite{Abadi2008-dd}. The reason we have chosen to elaborate on this algorithm is two-fold. First, the algorithm is well explained by Abadi \ea, and the paper contains detailed figures that help the understanding of how the algorithm works. Secondly, the algorithm uses some temporary data structures (bitmaps and hash tables) which we believe might be relevant structures for indexing or caching.

The \textit{invisible join} algorithm assumes a star schema style table, and works by rewriting joins into predicates on the foreign keys in the fact table. To improve cache performance, the algorithm minimizes the values that need to be extracted out of order, and to reduce memory traffic, it avoids unnecessary materialization steps. 

The \textit{invisible join} algorithm works in three phases:
\begin{enumerate}
  \item The smaller relations (dimension tables) are hashed, but if the tables are filtered with predicates, only matching keys are inserted into the hash table.
  \item Per hashmap generated in the first phase of the algorithm, the larger relation is probed, and a bitmap indicating qualifying is created. The resulting bitmap for each dimension table is combined using bitwise operations, like \texttt{AND} or \texttt{OR}. Creating these bitmaps might also happen in parallel. 
  \item To complete the join, keys in the foreign columns in the fact table are extracted using the bitmap from the previous step. In this step, either positional lookup (for sorted values) or the hash tables from the first step can be used to get the projected values.
\end{enumerate}
The entire algorithm is illustrated in Figure \ref{fig:invisible-join}.

We gave the \textit{invisible join} as an example join algorithm, because we believe a similar method is needed in a \bd~application with multiple tables. We see that hash tables from the first phase and bitmaps from the second phase are candidates for caching, a technique we saw in Section \ref{sec:Caching and Dynamic Index Generation} commonly used to improve read performance.



\section{Grouping and Aggregation}
\label{sec:Grouping and Aggregation}
Conceptually, grouping and aggregation is relatively straight forward: The record set is scanned and divided into groups based on one or more attributes, and for each group an accumulator is held \cite{Bratbergsengen2015-ed}. Accumulators can be of various types, which includes \term{count}, \term{min}, \term{max}, \term{average}, and \term{sum}. Some aggregates require just a single memory cell, like \term{count} and \term{sum}, others are more involved and require more memory to calculate, like \term{median} and \term{average}. Aggregated results are stored in a suitable structure, like a hash table or linear array. 

Multiple optimizations can be done to improve grouping and aggregation performance. First, the operations can be applied at the same time as for the scan \cite{Lemke2010-is}. This is done in \blink, where the core idea is a generalized scan that filters, groups, and aggregates on a single scan over the data \cite{Raman2008-gi}.

Dictionary encoding can simplify grouping and aggregation, since dictionary entries can be used directly for storing the aggregated results \cite{Boncz2005-wj, Lemke2010-is}. This is done in \monetx~and \blink. \blink~uses dictionary codes directly as group codes. Doing such results in a perfect hash \cite{Raman2008-gi}. Also, some aggregates, like minimum and maximum value in a column, can be evaluated scanning only  dictionary entries \cite{Lemke2010-is}.

\ffigure{img/vector-group-by}{The \term{Vector Group By} technique used in \oracle. Courtesy of \cite{Oracle2015-fs}.}{fig:vector-group-by}
On composite \texttt{GROUP BY} operations, a multidimensional structure may be used. \oracle~uses a technique which they call \term{Vector Group By}, which is a compact multipdimensional array for storing aggregate results. The operation is depicted in Figure~\ref{fig:vector-group-by}. \blink~calculates a minimal perfect hash in this case \cite{Raman2008-gi}.

\ffigure{img/parallel-group-by}{Courtesy of \cite{Raman2013-em}.}{fig:parallel-group-by}
If the columns are partitioned horizontally, grouping and aggregation must be performed in two phases: A scan phase per partition followed by a global merge phase \cite{Lemke2010-is}. In a distributed system, the grouping is even more involved. For \ibm, local grouping is performed per node in parallel \cite{Raman2013-em}\todo{elaborate}. In \exasol~and \oracle, an attribute in a table might specifiy table partitioning \cite{Exasol2014-xh, Lahiri2015-mz}. If a query is grouped by the same attribute, the work is already done.

As mentioned in Section \ref{sub:Queries}, in \bd~workloads, the groups will be predefined in the panel. However, new aggregation results must be populated when filters are applied.

\subsection{Parallel Grouping and Aggregation}
\label{sub:Parallel Grouping and Aggregation}
Vendors say their grouping operations scale linearly with the number of threads until the CPU is saturated \cite{Farber2012-vh}

In \ibm~, each thread performs a local grouping on a local hash tables and creates a linked list of overflow buckets \cite{Raman2013-em}. When they are full, they get published globally. Second phase, each thread reserves a partition to merge, and it merges all local hash tables to a global one.

\section{Query Optimization}
\label{sec:Query Optimization}
Our study of query optimization will be brief, but we have found some sources relevant for OLAP workloads. Most of the database systems found in literature use some kind of query optimization. 

In general, query optimizers must seek to reduce the search space \cite{Boncz2002-yj, Stonebraker2005-qz}. \blink~makes query optimization much easier, since the only operation for evaluating predicates in a query is a scan operation \cite{Barber2012-xt}. In addition, joins and grouping is done in prespecified order. The core idea of this system is a generalized scan which performs scan, selection, groping, and aggregation \cite{Raman2008-gi}. 

If compression is applied to the database, it is important that the optimizer is aware \cite{Westmann200-mz}. The query optimizer must be aware that columns might be compressed, and that these compressed columns can be worked on directly without decompression \cite{Stonebraker2005-qz}

In addition, it is important to filter data as early as possible in the plan \cite{Lamb2012-kg}, and in join operation  most selective columns should be processed first \cite{Holloway2008-rr}. In a parallel settiong, the query optimizer should minimize the need for coordination \cite{Exasol2014-xh}.

Long in-lists can be turned into a precomputed hash table \cite{Raman2013-em}.

We conclude our discussion on query optimization that query execution must utilize available optimizations allowed by the design. Metadata indexes (Section ?) can be used to skip blocks. Dictionaries can be checked, and partitions can be skipped entirely if the key is not present in the dictionary (Section ?). \texttt{LIKE} predicates can be turned into \texttt{IN} predicates based on the entries in the dictionary.

%\section{Query structures}
%\label{sec:Query structures}
%\missingfigure{Selection vector illustration from \cite{Boncz2005-wj}}
%If the queries work on vectors at a time (vectorized execution, section X), the auxiliary structure in \monetx~is a selection vector \cite{Boncz2005-wj}.

\section{Other Considerations}
\label{sec:Other Considerations}
We find one of the most important technique for good query performance to operate directly on compressed data, which we have studied in Section \ref{sub:Working Directly on Compressed Data}. As we have seen, this is especially important when using dictionary compression, such that most predicates can be performed by simple integer operations \cite{Abadi2008-dd}. If the data is bitpacked, queries can be processed in a SIMD-like fashion, a technique we study in more detail in Section \ref{sec:SIMD}. Systems querying direcly on compressed data include \cstore~\cite{Stonebraker-qz}, \ibm~\cite{Raman2013-em}, \mssql~\cite{Larson2013-mc}, \blink~\cite{Johnson2008-cp}, \sapnw~\cite{Lemke2010-is}. All of these systems claim one of their main benefits in terms of performance is their ability to work directly on the compressed data.


We see that some systems make a distinction between measure and dimension attributes \cite{Kamkolkar2015-iq, Johnson2008-cp}. A dimension attribute is typically answers questions like "where", "what", and "when", whereas a measure attribute nomally answers in terms of "how much" \cite{noauthor_undated-es}. Although we have no specific evidence of what this classification can do to aid query processing, we believe it can help determine data placement and which results that are cached. For dimensions, join indexes and filtered results are obvious candidates for caching. In a banked layout, used in \blink, or in column projections, used in \cstore, dimension attributes should be put together in the same bank or projection to aid predicate evaluation. Measure attributes on the other hand are candidates for caching of query results

\subsection{Query Restrictions}
\label{sub:Query Restrictions}
Most DBMSes support normal SQL operations. However, to simplify some of the queries, one may put restrictions to the SQL. Earlier versions of \mssql~did not support outer joins and group-by on scalar attributes \cite{Larson2013-mc}. This was justified as none of this was common for "datawarehouse scenarios".

Other work has suggested that standard SQL can be cumbersome and expensive \cite{Plattner2014-fr}, and want to investigate new ways of calculating and maintaining a database. \qlikview~is not in the \term{query-based paradigm}, and claims this is beneficial due to ... \todo{insert reasons here}

\bd~applications do not need to support all SQL operations which a DBMS normally does. For instance, joins can be limited to equi-joins over foreign keys, and aggregations may be limited to groups of one or more dimensions in the schema.

\section{Late materialization}
\label{sec:Late materialization}
Materialization is the word that is used when intermediate results are stored in memory. It can be used on many levels, where materialization can mean moving results from registers to memory \cite{Neumann2011-uq}, or can mean stiching tuples together for columns. Abadi \ea~\cite{Abadi2008-dd} have investigated the effects of late materialization, that is keeping the columns for as long as you can without materializing into rows. This optimization allows for 5\%-50\% performance boost depending on the query. \term{Late materialization} is used by several database systems, like \ibm~\cite{Raman2013-em} and \monetdb~\cite{Boncz2002-yj}. The latter has later been critiziced by materializing too much \cite{Boncz2005-wj}

Holloway \ea~\cite{Holloway2008-rr} has investigated the materialization size, and concluded that vectors of 100 was a good size. The reasoning behind this is that the buffer fits in the first level of cache, while still reducing overhead
\section{Chapter Conclusion}
\label{sec:Chapter Conclusion}
We conclude this chapter that join should be executed using a \term{nested loop} algorithm. This type of algorithm is suited for joins where tables fit in memory, which we assume. We also recommend adding Bloom filters to the structures to limit the number of keys entering the join. We recommend using a technique similar to Abadi \ea's \algmet{Invisible Join} for joining.

We also encourage investigating the benefits of caching temporary structures used in both cache and aggregation. In the case of \algmet{Invisible Join}, the predicate bitmaps can be cached.

Queries must also work directly on compressed data, as concluded in Chapter \ref{chp:Compression}.
