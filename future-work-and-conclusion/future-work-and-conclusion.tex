\chapter{Conclusion and Future Work}
\label{chap:Conclusion and Future Work}

\section{Conclusion}
\label{sec:Conclusion}
In this report, we set out to answer the following research question:

\textbf{RQ1: How to design a high-performance database software that is capable of supporting \bd~workloads on large datasets?} 

Our literature review has been successful in identifying several techniques, design concepts, algorithms, and other considerations needed for a high-performance \bd~aplication. Our findings are presented by category in a structured and clear way, and we have used several examples, figures, and specific algorithms to explain and increase the understanding of each topic. This way, we believe we have given an overview of the big picture of read-optimized in-memory databases and hopefully inspired further research.

We also set out to introduce new evidence for performance optimizations related specifically to \bd~products, because current products do not reveal how they work internally. For each topic presented in the report, we have tried to discuss our reference products, \qlikview~and \tableau, and how they relate to that particular subject. This way, we believe our report provides insight into how these applications work under the hood.

Below, we summarize the conclusions of each chapter:
\begin{itemize}
  \item In Chapter \ref{chap:Background Information}, we looked into \bi~and \bd~products. We saw that \bd~applications do not use a traditional query-based interface, but still need most of the functionality provided by SQL. We also saw that OLAP databases should be tested using both \textit{real-world data} and the standardized \textit{TPC-H benchmark}.

  \item In Chapter \ref{chap:Data Layout}, we saw how data layout can affect database performance, and we discussed row storage, column storage, partitioning, and alternative storage layouts. We pointed at \textit{column store with horizontal partitioning} as the most promising alternative for several reasons; columns require less memory traffic and are more compressible, and horizontal partitions enable parallelization and easy data pruning.

  \item In Chapter \ref{chap:Data Compression}, we saw how analytical queries benefit from compression due to a reduction in memory traffic and improved cache performance. We concluded that \textit{dictionary encoding with bitpacked columns and a sorted dictionary} is the most promising and versatile compression method. This method allows query operators to work directly with the compressed data, ideally in an SIMD-like fashion, and the sorted dictionary allows for efficient single value lookups and range predicate evaluation.
    
  \item In Chapter \ref{chap:Indexes and Auxiliary Structures}, we looked at how indexes and other data structures can increase performance in a read-optimized database. We concluded that \textit{bitmap indexes} and the usage of \textit{database statistics} are the most promising techniques in this chapter. We also saw how several systems increase performance by \textit{caching} and \textit{dynamic index generation}.

  \item In Chapter \ref{chap:Parallelization and Scaling}, we saw how parallelism should be applied on all levels for ultimate performance. We recommended looking into \textit{thread- and instruction-level parallelism}, as well as \textit{SIMD instructions}. We also recommended a \textit{scale-up} over a scale-out approach for simplicity and better cache performance.

  \item In Chapter \ref{chap:Hardware Utilization}, we saw that careful implementation is necessary to utilize available hardware. The implementation should \textit{keep the number of branches, function invocations and instruction dependencies at a minimum}, and algorithms should be made \textit{cache-aware}. We pointed at \textit{vectorized execution} as the most promising technique in this chapter.

  \item In Chapter \ref{chap:Query Processing}, we discussed basic query processing techniques like joins and grouping/aggregation. We concluded that \textit{nested based loop} join algorithms are suited for in-memory, parallel databases. For grouping on single keys, the \textit{dictionary should be used}, and for grouping on multiple keys, a \textit{two-pass solution} should be considered. Systems should also be faithful to the \textit{late materialization} principle.

  \item In Chapter \ref{chap:Other Topics}, we discussed how to relax the read-only and the in-memory assumptions. We also concluded that table \textit{denormalization should be avoided}. Last, but not least, we emphasized the importance of \textit{proper application design}.
\end{itemize}

\section{Future Work}
\label{sec:Future Work}
As stated in the introduction, answering \textbf{RQ1} is seen as the first step towards a bigger goal:

\textbf{G1: Implement \bd~capabilities in \genusSoftware~ that has high performance, handles large datasets, and utilizes the available hardware. The new product must be competitive to other \bd~products regarding performance and functionality.}

It is, therefore, interesting to discuss how our newly acquired knowledge can be applied to reach this goal.

In this report, we have conveniently left \genusSoftware~out of the discussion to avoid bias and get a more general understanding of high-performance, read-optimized databases. However, future work will require a thorough analysis of \genusSoftware~and the existing \bd~implementation to identify opportunities, constraints, and limitations. In this step, we expect to identify optimization techniques from this report that looks promising and can be applied to \genusSoftware.

From an academic standpoint, future work has two compelling research angles. First, one or several optimization techniques can be studied in detail. Such research could include testing of different implementations on different workloads to gain a better understanding of the specific technique. It would also be interesting to see how different optimizations affect each other. An example of such work could be to study the effect of column storage with and without horizontal partitioning, or with and without data compression.

The other angle could be to dive into \bd~specific optimizations. In this report, we have seen that \bd~products are different from general-purpose DBMSes because they, for instance, hold an application state and do not use a query-based interface. Could these differences be exploited and affect which design concepts and optimizations that are most efficient and practical? An example of such research could be to investigate the effects of bitmap indexes from a \bd~perspective, and see if they have more use cases in a \bd~application than they have in a general purpose DBMS.

A draft of a research plan addressing both research angles can be found in Appendix A. This plan was submitted in \textit{TDT39 - Empirical Research Methodology} and can be used as an example of how to continue towards \textbf{G1}. However, it has been pointed out that this plan is overly ambitious for the scope of this research.
