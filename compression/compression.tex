\chapter{Data Compression}
\label{chap:Data Compression}
When data is stored in a format optimized for analytical workloads, the next step in the process of improving performance is to apply compression. We saw in the last chapter that one of the benefits of column storage is the compressibility. 

In this chapter, we study compression in more detail. We first discuss compression in general, compression types, and how queries can be processed using compressed data directly. We then look into more specific algorithms: \bp, \de, \dele, and \rle.

\newpage

\section{Compression}
\label{sec:Compression}
Historically, compression has been thought of a measure for reducing disk usage and memory footprint. However, in our case, compression of data also comes with the benefit of increased performance. A study executed by Abadi \ea~\cite{Abadi2008-dd} looked into database compression for in-memory databases, and concluded compression increases performance by a factor of two on average.

Among systems we have studied in this research, we have identified several that applies compression for performance reasons. Among these systems are \ibm~\cite{Raman2013-em}, \cstore~\cite{Stonebraker2005-qz}, \vertica~\cite{Lamb2012-kg}, \oracle~\cite{Oracle2015-fs}, and \gorilla~\cite{Pelkonen2015-ko}. In addition to this, our reference products \tableau~and \qlikview~have also use compression extensively to achieve good performance \cite{Kamkolkar2015-iq, Qlik2014-vd}. Finally, we see that the best performing DBMS in the TPC-H benchmark applies compression to its data, using structures that are designed for rapid retrieval.

Compression is beneficial for performance due to:
\begin{itemize}
  \item Cache locality is improved \cite{Exasol2014-xh}. More values from a single column (or record) fit in the cache at the same time.
  \item Memory traffic is reduced. Compression can help in turning a program from memory-bound to CPU bound \cite{Willhalm2009-hu}.
  \item Compression can reduce CPU cycles \cite{Stonebraker2005-qz}. First of all, as stated above, it reduces memory latency and improves cache locality, such that more cycles can be used for calculations and not waiting for memory. However, compression also enables working on multiple data in parallel using SIMD instructions, as we see in Section \ref{sec:SIMD}.
\end{itemize}

Still, even though compression is used to increase database performance, the fact that compression reduces memory usage is also important. Even though DRAM is cheap, it is still rarely over-provisioned and unused \cite{Barber2014-ey}. Also, compressed data frees up space for other things, like indexes and result caches. For instance, \oracle~justifies their dual format by using the space freed up by compressing the columns \cite{Lamb2012-kg, Lahiri2015-mz}.

\subsection{Compression Types and Light-Weight Compression}
\label{sub:Compression Types and Light-Weight Compression}
A study of compressed databases executed by Westmann \ea~investigates database compression, and concludes that that the compression must be light-weight \cite{Westmann2000-mz}. In other words, the real benefit of compression can only be leveraged if the decompression effort can be minimized \cite{Lemke2010-is}. Light-weight compression has been defined by Holloway \ea~as \bp, \de, \dele, and \rle~\cite{Holloway2008-rr}. Holloway \ea~also concludes that \de~and \rle~ar the best compression schemes for column stores. These compression techniques are fast and fine-grained, which is important for performance \cite{Lemke2010-is}.

Zukowski \ea~concludes that a compression algorithm should care about super-scalar processors. This implies that the algorithm should be able to pipeline loops, support out-of-order execution, and avoid if-then-else in inner loops \cite{Zukowski2006-oz}. We study these techniques in greater detail in the chapter about hardware utilization (Chapter \ref{chap:Hardware Utilization}). 

A database can use multiple compression schemes. First, most of the light-weight compression techniques can be combined, where the most common combination is \de~and \bp. We study this idea in Section \ref{sec:Dictionary Encoding}. Second, different compression schemes can be used for different columns. In Section \ref{sub:Sorting}, we saw that \cstore~and \vertica~allow for multiple column projections (a subset of the columns), where each projection is sorted based on one of the columns in the subset. This results in each projection having one \textit{self-order} column and several \textit{foreign-order} columns.  \cstore~and \vertica~use the following guidelines when deciding which encoding to use \cite{Stonebraker2005-qz}: 

\begin{itemize}
  \item \textbf{Self-order, few distinct values:} \rle~is used for these columns, where each value is stored once together with how many times the value occurs.
  \item \textbf{Foreign-order, few distinct values:} Each value in the column is stored in conjunction with a bitmap indicating the positions in which the value is stored. Since the bitmaps typically are sparse, \rle~can be applied to the bitmaps to save space.
  \item \textbf{Self-order, many distinct values:} \dele~is used. The idea of this scheme is to save the values as a delta from the previous value in the column.
  \item \textbf{Foreign-order, many distinct values:} Columns are left uncompressed
\end{itemize}

Later in this chapter, we elaborate on the situations where the various compression schemes are beneficial.

\subsection{Working Directly on Compressed Data}
\label{sub:Working Directly on Compressed Data}

\afigure{img/ram-cache-decompression.png}{I/O-RAM vs RAM-CPU compression. In the left sub-figure, data is decompressed before it is put in the buffer manager (RAM). In the right sub-figure, data is kept compressed in the buffer manager and only decompressed when it is brought into the CPU cache. Courtesy of \cite{Zukowski2006-oz}.}{fig:ram-cache-decompression}{0.6}

Earlier, if compression was applied to a disk-based database, the data was decompressed when brought up to RAM. In 2006, Zukowski \ea~suggested that data should not be decompressed when moved from disk to RAM, but when brought from RAM to cache \cite{Zukowski2006-oz}, as depicted in Figure \ref{fig:ram-cache-decompression}. \monetx~is a system that decompresses when data is moved from RAM to cache \cite{Johnson2008-cp}.

However, the most performance benefit of compression is seen when the system works on the compressed data directly \cite{Lemke2010-is}. This implies that data should not be decompressed until it is materialized and sent to the user. This is backed by the creators of \blink, who says data should never be decompressed before absolutely needed \cite{Barber2012-xt}. \oracle~claims one of the main performance benefits is to work directly on the compressed data \cite{Oracle2015-fs}. Not decompressing data before it is needed relates to a technique known as \textit{late materialization}, a technique we study in Section \ref{sec:Late Materialization}.


\section{Bitpacking}
\label{sec:Bitpacking}
\afigure{img/bitpacking.png}{Bitpacked column values. Values are stored with no more bits than needed to represent the column, which results in values that are not aligned to machine word boundaries. They may be spread across several machine words and share their machine word(s) with other codewords. Courtesy of \cite{Willhalm2013-ri}.}{fig:bitpacking}{0.8}

\bp~is a trivial form of compression, where values are stored with no more bits than needed. In other words, if a column has a cardinality of 32, only 5 bits are required to represent a value. This way, in a 32-bit architecture, 100 values can be stored using $500$ bits, and not $100*32 = 3200$. \bp~has lower compression than algorithms that allow variable length, but values can be randomly accessed in constant time \cite{Raman2008-gi, Willhalm2013-ri}. Also, \bp~enables SIMD processing, which we discuss in Section \ref{sec:SIMD}. \bp is well suited for high cardinality, uniform distribution of values, where dictionary encoding only provides an unnecessary layer of indirection \cite{Holloway2008-rr}.

As seen in Figure \ref{fig:bitpacking}, bitpacked values will not generally align to word boundaries. For processing, values normally have to be moved to the word boundary, but this cost has been found to be negligible \cite{Holloway2008-rr}. Aligning values can be done in an SIMD like fashion, a technique we study in Section \ref{sec:SIMD}.

In its simplest form, \bp~works directly on the column data, but \bp~can be more powerful if combined with other compression types. We see in Section \ref{sec:Dictionary Encoding} that dictionary keys can be bitpacked, and in Section \ref{sec:Delta Encoding} that Delta Encoding benefits from \bp~if the deltas between the values are small. Also, bitpacked vectors can be used in an inverted index structure \cite{Schwalb2014-hn}, which we study in Section \ref{sec:Inverted Indexes}.

\subsection{Issues with Bitpacking}
\label{sub:Issues with Bitpacking}
\afigure{img/partitioned-bitpack.png}{A normal bitpacked vector (left) and a partitioned bitpacked vector (right). Instead of rebuilding the entire vector on value overflow, the partitioned bitpacked vector has different partitions where each partition is compressed using an increasing number of bits. Courtesy of \cite{Faust2015-ke}.}{fig:partitioned-bitpack}{0.9}
There are two major limitations with \bp~\cite{Faust2015-ke}. The first is that if the bitpacking overflows, the full bitpacked vector must be rebuilt. What this means is that if all values are mapped, such that there are no available values with $n$ bits, a new bit must be introduced, and the entire vector must be rebuilt where each value has $n + 1$ bits. This is depicted in the left part of Figure \ref{fig:partitioned-bitpack}. To counter this effect, Faust \ea~have suggested a partitioned bitpacked vector structure that creates a new partition with $n + 1$ bits on overflow, as seen in the right part of Figure \ref{fig:partitioned-bitpack}. Although this technique improves performance on insert operations, read performance suffers due to the extra overhead of looking up a value.

The second limitation with \bp~is that it does not account very well for data skew. In \bp, each distinct value in a vector contributes to the total number of bits required, completely disregarding the distribution of the values. Often in a database, there are a large number of distinct values, but they are not uniformly distributed. This problem can be solved with the partitioned vectors explained in the previous paragraph, by mapping the values that occur more frequently to the partitions with the fewest bit per value. Other algorithms map outliers to a separate structure, like \pfdelta~\cite{Bjorklund2011-wh}.


\section{Dictionary Encoding}
\label{sec:Dictionary Encoding}
\de, or \term{Dictionary Compression}, is widely used within column store databases. Systems that use \de~include \oracle~\cite{Lahiri2015-mz}, \ibm~\cite{Raman2013-em}, \saph~\cite{Farber2012-vh}, \sapnw~\cite{Lemke2010-is}, \blink~\cite{Johnson2008-cp}, \mssql~\cite{Larson2013-mc}, and more. \qlikview~stores each distinct data point only once \cite{Qlik2011-ef}. \tableau~does not mention anything about \de~in their whitepapers, but an official blog post claims that this compression technique is used \cite{noauthor_undated-us}.

\afigure{img/dictionary-sorted.png}{A sorted dictionary. Each distinct value is stored only once in the dictionary. A key is assigned to each entry in the dictionary, and those keys are stored in the columns instead of the actual values. Courtesy of \cite{Psaroudakis2014-ma}.}{fig:dictionary-sorted}{0.4}

In a dictionary encoded column, each distinct value is stored once in a structure known as \textit{the dictionary}. Keys are assigned to each entry in the dictionary, most commonly integers from zero and up. Columns store these keys and not the actual values, and data is compressed since each unique value is stored exactly once. \de~using a sorted dictionary is illustrated in Figure \ref{fig:dictionary-sorted}. \de~is particularly effective when a column in a column store has only a few distinct values in a large dataset \cite{Faust2015-ke}.


Except from the compression, one of the major advantages of \de, is that many database operations can be performed directly on the encoded values \cite{Faust2015-ke}, which we have seen in Section \ref{sub:Working Directly on Compressed Data} is important to achieve good performance. Integer comparison is less expensive than comparing the actual values, especially for strings. Additionally, range and \texttt{LIKE} predicates can be turned into \texttt{IN}-list operations, since the dictionary can be scanned first to find the relevant integer keys \cite{Barber2012-xt}.

If the columns are partitioned horizontally, which we have discussed in Section \ref{sec:Horizontal Partitioning}, it is common that each partition has a separate dictionary. This is the case for most database systems, like \oracle~\cite{Lahiri2015-mz}, \blink~\cite{Barber2012-xt}, and \mssql~\cite{Larson2013-mc}. When a dictionary is stored per partition, it can be used for quick data pruning; if a value is not present in the dictionary, the partition can be skipped. \blink~and \monetx~use this technique \cite{Barber2012-xt, Boncz2005-wj}. 

When implementing \de, special considerations should be taken. First, if the dictionary turns out bigger than the values it is replacing, \de~should not be used \cite{Holloway2008-rr}. Secondly, \de~performs best if the dictionary fits inside the L2 cache.

Like \bp, \de~does not handle data skew very well, since each unique value needs an entry in the dictionary no matter frequent that value appears in a column. Besides, for high cardinality columns, the compression is less efficient.

\subsection{Sorted Dictionaries}
\label{sub:Sorted Dictionaries}
Dictionaries can be either sorted or unsorted. Using a sorted dictionary enables easier value lookup using a binary search. However, more important for us, is that using a sorted dictionary can turn range scans into simple integer comparisons \cite{Faust2015-ke}. For instance, if we want to find all sales in 2010, we only need to look up the integer codes for January 1st, 2010 and January 1st, 2011 and find all integers within this range. Since integer comparisons are fast and effective, this technique will usually improve performance. 

Most database systems today use sorted dictionaries. \saph~is an example of such system \cite{Farber2012-vh}.

However, as briefly mentioned in Section \ref{sub:Sorting}, keeping a dictionary sorted implies a higher overhead on database inserts, updates and deletes. We see in Section \ref{sub:Delta Store} that some systems divide their data into two stores: A read-optimized store, and a delta store (for updates and inserts). With this division, it is common to use a sorted dictionary for the read-optimized store and an unsorted dictionary for the delta store \cite{Plattner2014-fr}.

\subsection{Dictionary Encoding and Bitpacking}
\label{sub:Dictionary Encoding and Bitpacking}
\de~is commonly used in conjunction with \bp. With this combination, the dictionary keys in the columns are stored with no more bits than necessary. Since dictionary keys normally are integers from zero to the number of entries, bitpacking enables high compression rates, especially for low-cardinality columns.

Systems using \de~and \bp~include \ibm~\cite{Raman2013-em}, \blink~\cite{Barber2012-xt}, \sapnw~\cite{Willhalm2009-hu}, and \saph~\cite{Psaroudakis2014-ma}. \qlikview~has also reported to compress data with only the number of bits required \cite{Qlik2014-vd}. \mssql~does not apply \bp~on dictionary keys in columns, and instead store them as 32-bit integers \cite{Larson2013-mc}.

The same advantages and disadvantages of \bp~apply to \de~with bitpacked columns. For instance, values can be looked up in constant time and queries can be processed in an SIMD-like fashion. However, insertions to the dictionary might lead to an overflow, which requires the entire column to be rebuilt.


\section{Delta Encoding}
\label{sec:Delta Encoding}

\ffigure{img/delta-encoding.png}{\dele. The difference between the current and the previous value is stored instead of the actual value. Since the difference between values usually is smaller than the actual values, fewer bits can be used to store the data. Courtesy of \cite{Victor_Lavrenko2014-hv}.}{fig:delta-encoding}

\dele, or \term{Delta Compression}, is a compression method where the difference between the previous and the current value in a column is stored instead of the actual value \cite{Wikipedia_contributors2015-cb}. This is illustrated in Figure \ref{fig:delta-encoding}.  Systems using this form for encoding include \vertica~\cite{Lamb2012-kg}, \cstore~\cite{Stonebraker2005-qz}, and \blink~\cite{Raman2008-gi}. 

\dele~is particularly effective when there are small differences between consecutive values in a column. In \cstore~and \vertica, \dele~is the compression of choice for sorted columns with high cardinality, since sorted columns minimize the deltas. \dele~can also be effective on sorted, low-cardinality columns, but is in general outperformed by \rle~in this case.

One of the major drawbacks of \dele~is that values cannot be accessed in constant time. To find a particular value at index $i$, all the values from $0$ to $i - 1$ must be decoded and accumulated. In addition, delta compressed columns are hard to work on directly without decompression.

\subsection{Delta Encoding and Other Compression Techniques}
\label{sub:Delta Encoding and Other Compression Techniques}
\bp~can be used in conjunction with \dele. If the deltas are small, which they typically are if applied to a sorted column, \bp~can compress a column significantly. The column can be compressed even further by using \de, \dele, and \bp~a the same time because the variance in the column values will be reduced when replaced with dictionary keys. However, applying \dele~to bitpacked, dictionary encoded columns comes at a cost; Queries can no longer work directly on the compressed data using simple integer operations. In other words, we improve compression, but at the expense of performance reduction.

An algorithm that combines \dele~with \bp~is \pfdelta \cite{Bjorklund2011-wh}. This algorithm handles outliers very well because a separate structure is used to handle them. \pfdelta~has the advantage of branchless compression and decompression, which we see in Chapter \ref{chap:Hardware Utilization} is important for performance.

\ffigure{img/timestamp-compression.png}{Compression of data stream (a) with timestamps and float values in \gorilla. Timestamps are stored as delta of deltas (b). The float values are \texttt{XOR}ed with the previous value  to extract meaningful bits (c). Courtesy of \cite{Pelkonen2015-ko}.}{fig:timestamp-compression}

If the differences between values are large, but the deltas are similar to each other, compression can be increased by storing the difference between the deltas (delta of deltas) instead. This is seen in Figure \ref{fig:timestamp-compression}b. \gorilla~uses this technique to store timestamps for data that arrive roughly at a fixed interval \cite{Pelkonen2015-ko}. Floating point numbers can be compressed by extracting and storing the different bits with an \texttt{XOR} operation of two consecutive numbers, as seen in Figure \ref{fig:timestamp-compression}c.

\section{Run-Length Encoding}
\label{sec:Run-Length Encoding}
\ffigure{img/rle.png}{\rle~on a sorted value range. Courtesy of \cite{Stoimen_undated-js}.}{fig:rle}
\rle~is a lossless data compression algorithm that replaces repeating data values with only one instance of the data and a number of how many times that value appear in the sequence \cite{Stoimen_undated-js}, like illustrated in Figure \ref{fig:rle}. Although it can be used for any sequence, \rle~works best on sorted data \cite{Bjorklund2011-wh, Holloway2008-rr}. In this case, each unique value in the sequence is represented exactly once, and this implies that \rle~works best for columns with low cardinality. \rle~is used in \cstore~\cite{Stonebraker2005-qz}, \vertica~\cite{Lamb2012-kg}, \oracle~\cite{Oracle2015-fs}, and \sapnw~\cite{Lemke2010-is}, but only if the values are sorted. \rle~can also be applied to compress sparse bitmaps \cite{Stonebraker2005-qz}.

Queries may benefit from \rle~on sorted columns when performing grouping and aggregation, since values are already put in groups. By keeping a prefix sum over all values, row ranges can easily be extracted for equality and range predicates. This is a form of letting the system work directly on the compressed data. \todo{Find source, rephrase}

%For sorted values, Run-length encoding can be used \cite{Bjorklund2011-wh}. A typical bla bla. The work by Holloway \ea~\cite{Holloway2008-rr} writes that run-length encoding works best on sorted data.

%Run-length encoding is used extensively in \cstore~and \vertica~\cite{Barber2012-xt} (which is the commercialization of \cstore). These databases allows for projections that are sorted, and will apply run-length encoding on columns with low cardinality that are sorted.

\section{Other Compression Techniques}
\label{sec:Other Compression Techniques}
One way of compressing a column is to use bitmaps \cite{Stonebraker2005-qz} \todo{index?}. By representing each column value as a bitmap, where the 1s in the map indicate which rows the value is present, compression can be achieved. Compression is best on unsorted columns with few distinct values. Since these bitmaps are typically sparse, they can be compressed further with techniques like \rle, which we study in Section \ref{sub:Compression of Bitmaps}.

QlikView uses a hash table to compress the data \cite{Qlik2014-vd}, and it yields a compression rate of 20\%-90\% dependent on the data.

\section{Chapter Conclusion}
\label{sec:Chapter Conclusion}
We have in this chapter concluded that compression is required in a \bd~application. Not only does it reduce memory footprint, but it increases performance by reducing the number of CPU cycles\todo{SEB 30/11: ?}, improve cache performance, and helps overcome the memory wall \todo{define this word}. Several database vendors claim compression is key for performance, including the top performing database in the \tpch, \exasol. To fully leverage the performance benefits of compression, compression must be light-weight and the queries must be processed directly on the compressed data. \todo{Introduce new stuff}

With this is mind, we conclude that \de~in combination with \bp~is the most versatile and promising compression algorithm, and it does not require the columns to be sorted. We do however suggest using a sorted dictionary. One of the disadvantages with this approach is that it does not handle inserts and updates very well, but since we focus on a read-only system, this does not apply. If the values are sorted, and the column have low cardinality, \rle~should be considered, since this scheme also allows for executing queries directly on the compressed data. Applying \dele~can also be considered if the values are sorted, but only if reducing the memory footprint is more important than performance.



%\oracle~utilizes a compression format on their in-memory storage layout \cite{Oracle2015-fs}. Compression is normally considered as a space-saving mechanism, but compression will also improve query performance. All scanning and filtering operations operate directly on the compressed columns which decreases query execution time. Data is decompressed before it is required for the result set. \oracle uses Dictionary Encoding, Run Length Encoding and Bit-Packing is used \cite{Oracle2015-fs}. 
