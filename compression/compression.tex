\chapter{Data Compression}
\label{chap:Data Compression}
When data is stored in a format optimized for analytical workloads, we find the next step in the process of improving performance to apply compression. Compression needs to be light-weight such that one typle can be decompressed at a time. Dictionary compression is the most common compression scheme due to simplicity in predicate evaluation, but delta and run-length encoding may also be used.
\newpage

\section{Compression}
\label{sec:Compression}
To improve cache locality, reduce memory footprint, and hence increase performance, the most prevalent technique used is compression of data. It is used by nearly all database systems, like the \ibm~\cite{Raman2013-em}, \cstore~\cite{Stonebraker2005-qz}, ~\pn{Vertica} \cite{Lamb2012-kg}, \oracle~\cite{Oracle2015-fs} and more. Reference products \tableau~and \qlikview~has also reported to use compression extensively to achieve good performance \cite{Kamkolkar2015-iq, Qlik2014-vd}. Looking at \exasol, the highest performing database system in the \tpch~does also utilize highly compressed data, structured for rapid retrieval \cite{Exasol2014-xh}.

Historically, compression has been thought of a measure for reducing disk and memory footprint. However, in this setting, compressing the data also comes with the benefit of increased performance. Abadi \ea~\cite{Abadi2008-dd} explores column compression and how it affects performance. In this research, they found a performance increase of two when applying compression, but this was dependent on the query. Compression cause common database operations to go from memory to CPU bound, hence used to overcome the memory wall \cite{Willhalm2009-hu}, and it can be used to reduce CPU cycles \cite{Stonebraker2005-qz}. Compression is better suited for read-only functionality \cite{Westmann2000-mz}.

Of more recent work databases, the \gorilla~\cite{Pelkonen2015-ko} claims that compression is one of the main reasons for good performance. The \exasol~\cite{Exasol2014-xh} writes that data is compressed of two reasons. First, it reduces the total storage, and second; it increases performance by reducing pressure on the CPU caches.

There are several sources that indicates that Columns are more easily compressible \todo{insert references to this here}. \mssql~\cite{noauthor_undated-vq} claim a compression rate of 10x when storing in a columnar format. However the work by Holloway \ea~\cite{Holloway2008-rr} contradicts this and claims that row stores can be compressed just as much if done correctly. We believe that even though row storage is as compressible as column store, compression of columns are simpler and more practical.


Still, even though compression is used to increase database performance, the fact that compression reduces memory usage is also important. Even though DRAM is cheap, it is still rarely over-provisioned and unused \cite{Barber2014-ey}. In addition, compressed data frees up space for other things, like inverted indexes and cached results. \oracle~justifies their dual format by using the space freed up by compressing the columns \cite{Lamb2012-kg, Lahiri2015-mz}.

\subsection{Compression Types and Light-Weight Compression}
\label{sub:Compression Types and Light-Weight Compression}
A study of compressed databases executed by Westmann \ea~investigates database compression, and concludes that that the compression must be light-weight \cite{Westmann2000-mz}. In other words, the real benefit of compression can only be leveraged if the decompression effort can be minimized \cite{Lemke2010-is}. Light-weight compression has been defined by Holloway \ea~as \bp, \de, \dele, and \rle~\cite{Holloway2008-rr}. Holloway \ea~also concludes that \de~and \rle~ar the best compression schemes for column stores. These compression techniques are fast and fine-grained, which is important for improving performance \cite{Lemke2010-is}.

Zukowski \ea~concludes that a compression algorithm should care about super-scalar processors, that is it should be loop pipelineable, support out-of-order exectuion, and avoid if-then-else in inner loops \cite{Zukowski2006-oz}. 

A database can pick multiple compression schemes. First, most of the light-weight compression techniques can be combined, where the most common combination is \de~and \bp. We study this idea in Section \ref{sec:Dictionary Compression}. Second, different compression schemes can be used for different columns. \cstore~uses the following guidelines when deciding which encoding to use \cite{Stonebraker2005-qz}:
\begin{itemize}
  \item \textbf{Self-order, few distinct values:} \rle~is used for these columns, where each value is stored in once together with how many times the value occurs.
  \item \textbf{Foreign-order, few distinct values:} Each value in the column is stored together with a bitmap indicating the positions in which the value is stored. Since the bitmaps typically are sparse, \rle~can be applied to the bitmaps to save space.
  \item \textbf{Self-order, many distinct values:} \dele~is used. The idea of this scheme is to save the values as a delta from the previous value in the column.
  \item \textbf{Foreign-order, many distinct values:} Columns are left uncompressed
\end{itemize}

We look into the different compression schemes later in this chapter, and see how they can be combined. We will see when the various compression schemes are beneficial.

\subsection{Working Directly on Compressed Data}
\label{sub:Working Directly on Compressed Data}
\afigure{img/ram-cache-decompression.png}{I/O-RAM vs RAM-CPU compression. Courtesy of \cite{Zukowski2006-oz}.}{fig:ram-cache-decompression}{0.6}
Historically, if compression was applied to a disk based database, the data was decompressed when brought up to RAM. In 2006, Zukowski \ea~suggested that data should not be decompressed when moved from disk tor RAM, but when brought from RAM to cache. See Figure \ref{fig:ram-cache-decompression}. \monetx~is a system that decompress when data is moved from RAM to cache \cite{Johnson2008-cp}.

However, the most performance benefit is gained by working on compressed data directly \cite{Lemke2010-is}. This implies that data should not be decompressed until it is materialized and sent to the user. This is backed by the creators of \blink, who says data should never be decompressed before absolutely needed \cite{Barber2012-xt}. This technique is known as late materialization, which we will look closer into in Section \ref{sec:Late Materialization}. \oracle~claim one of the main performance benefits is to work directly on the compressed data \cite{Oracle2015-fs}.


\section{\bp}
\label{sec:Bitpacking}
\afigure{img/bitpacking.png}{Bitpacked column values. Values are stored with no more bits than needed to represent the column, which results in values that are not aligned to machine word boundaries. They may be spread spread across several machine words and share their machine word(s) with other codewords. Courtesy of \cite{Willhalm2013-ri}.}{fig:bitpacking}{0.8}

\bp~is a trivial form for compression, where values are stored with no more bits than really needed. In other words, if a column has a cardinality of 32, only 5 bits are needed to represent a value, instead of using the full data word, which normally is 32 or 64 bits. \bp~has lower compression than algorithms that allow variable length, but values can be randomly accessed in constant time \cite{Raman2008-gi, Willhalm2013-ri}. Bitpacking is well suited for high cardinality, uniform distribution of values \cite{Holloway2008-rr}.

As seen in Figure \ref{fig:bitpacking}, bitpacked values will not generally be aligned to word boundaries. For processing, values normally have to be moved to the word boundary, but this cost has been found to be neglible \cite{Holloway2008-rr}. Aligning values can be done in a SIMD like fashion, a technique we study in Section \ref{sec:Single Input Multiple Data}.

In its simplest form, \bp~works directly on the column data, but \bp~can be more powerful if combined with other compression types. We see in Section \ref{sec:Dictionary Encoding} that dictionary keys can be bitpacked, and in Section \ref{sec:Delta Encoding} that Delta Encoding benefit from \bp~if the deltas between the values are small. In addition, bitpacked vectors can be used in an inverted index structure \cite{Schwalb2014-hn}.

\subsection{Issues with \bp}
\label{sub:Issues with Bitpacking}
\afigure{img/partitioned-bitpack.png}{A normal bitpacked vector (left) and a partitioned bitpacked vector (right). Instead of rebuilding the entire vector on value overflow, the partitioned bitpacked vector has different partitions where each partition is compressed using an increasing number of bits. Courtesy of \cite{Faust2015-ke}.}{fig:partitioned-bitpack}{0.9}
There are two major limitations with \bp~\cite{Faust2015-ke}. The first is that if the bitpacking overflows, the full bitpacked vector must be rebuilt. What this means in practice is that if all values are mapped, such that there are no available values with $n$ bits, a new bit must introduced, and the entire vector must be rebuilt where each value uses $n + 1$ bits. This is depicted in the left part of Figure \ref{fig:partitioned-bitpack}. To counter this effect, Faust \ea~have suggested a partitioned bitpacked vector structure that creates a new partition with $n + 1$ bits on overflow, as seen in Figure \ref{fig:partitioned-bitpack} \cite{Faust2015-ke}. Although proved to improve performance on insert operations when vector rebuilding is considered, read performance suffers due to the extra overhead of looking up a value.

The second limitation with \bp~is that it does not account very well for data skew. In \bp, each distinct value in a vector contributes to the total number of bits required, completely disregarding the distribution of the values. Often in a database, there are a large number of distinct values, but actually a very low number of those values accounts for more than 90\% \cite{Faust2015-ke}. This problem can be solved with the partitioned vectors explained in the previous paragraph, by mapping the values that occur more frequently to the partitions with the fewest bit per value. Other algorithms map outliers to a separate structure, like \pfdelta~\cite{Bjorklund2011-wh}.

\section{\de}
\label{sec:Dictionary Encoding}
\de, or \term{Dictionary Compression}, is widely used within column store databases. \de~is used in \oracle~\cite{Lahiri2015-mz}, \ibm~\cite{Raman2013-em}, \saph~\cite{Farber2012-vh}, \sapnw~\cite{Lemke2010-is}, \blink~\cite{Johnson2008-cp}, \mssql~\cite{Larson2013-mc}, and more. \qlikview~stores each distinct data point only once \cite{Qlik2011-ef}. This is confirmed in a blog post at \textit{DBMS2}\footnote{http://www.dbms2.com/} \cite{noauthor_undated-js}. \tableau~does not mention anything about \de~in their whitepapers, but an official blog post claim that this technique is used \cite{noauthor_undated-us}.

\afigure{img/dictionary-sorted.png}{A sorted dictionary. Courtesy of \cite{Psaroudakis2014-ma}.}{fig:dictionary-sorted}{0.4}
In \de, data is compressed by storing all the distinct values in a dictionary, such that each data point contains a pointer to the dictionary entry, and not the value itself. \de~using a sorted dictionary is illustrated in Figure \ref{fig:dictionary-sorted}. \de~is particularly effective when a column in a column store has only a few distinct values in a large dataset \cite{Faust2015-ke}.

One of the major advantages with using \de, is that many database operations can be performed directly on the encoded values \cite{Faust2015-ke}, which we have already seen in Section \ref{sec:Working Directly on Compressed Data} is key to achieve good performance. Integer comparison is generally less expensive than comparing the actual value, especially when the actual values are strings. Additionally, range and \texttt{LIKE} predicates can be turned into \texttt{IN}-list operations, since the dictionary can be scanned first to find matching integer key \cite{Barber2012-xt}

If the columns are partitioned horizontally, which we have discussed in Section \ref{sec:Horizontal Partitioning}, it is common that each partition has its own dictionary. This is the case for most database systems, like \oracle~\cite{Lahiri2015-mz}, \blink~\cite{Barber2012-xt}, and \mssql~\cite{Larson2013-mc}. When a dictionary is stored per partition, the dicitonary can be used for data pruning. If a key is not present in a dictionary associated with a partition, we know the value is not present in than partition. This technique is used by \blink~and \monetx~\cite{Barber2012-xt, Boncz2012-xt}.

Special considerations should however be taken when implementing dictionary encoding. First of all, the dictionary should fit inside the L2 cache, and it should not be used if the dictionary turns out bigger than the value it is replacing \cite{Holloway2008-rr}. 

Like \bp, \de~does not account very much for data skew, since each unique value need an entry in the dictionary no matter how often that value is present in the column. In addition, if the column consist of almost exclusively unique values, the dictionary compression scheme does not apply that much compression, and only applies an extra layer of indirection. \todo{Add more disadvantages about dictionary encoding}.

\subsection{Sorted Values}
\label{sub:Sorted Values}
Dictionaries can be either sorted or unsorted. Using a sorted dictionary enables easier value lookup using a binary search, but more importantly is perhaps that range scans can easily be turned into simple integer comparisons because the integers are ordered \cite{Faust2015-ke}. When for instance we want to find all sales in 2010, we only need to look up the integer codes for January 1st 2010 and January 1st 2011, and find all integers within this range. Sorted dictionaries are used by most database systems today. \saph~is an example of such system \cite{Farber2012-vh}.

However, as briefly mentioned in Section \ref{sub:Sorting}, a keeping a dictionary sorted implies a higher overhead on database inserts, updates and deletes. Therefore, some systems use a sorted dictionary for the read-optimized store, but an unsorted one for the delta store \cite{Plattner2014-fr}.  We look into delta stores in Section \ref{sub:Delta Store}.

\subsection{\de~and \bp}
\label{sub:Dictionary Encoding and Bitpacking}
It is common to use \de~in conjunction with \bp. Using this combination, the dictionary keys stored in the columns are stored with no more bits than neccesary. Since no integers are higher than the number of keys present in the dictionary, this scheme allows for highly compressed values. Systems using this scheme include \ibm~\cite{Raman2013-em}, \blink \cite{Barber2012-xt}, \sapnw~\cite{Willhalm2009-hu}, and \saph~\cite{Psaroudakis2014-ma}. \mssql~does not apply \bp~on the column values \cite{Larson2013-mc}. \qlikview~has also reported to compress data with only the number of bits required \cite{Qlik2014-vd}

The same advantages and disadvantages of \bp~applies to \de~with bitpacked columns. In addition to the increased compression rate, values can be looked up in constant time and queries can be processed in a SIMD-like fashion. However, insertions to the dictionary might lead to overflow, which requires the vector to be rebuilt.


\section{\dele}
\label{sec:Delta Encoding}
\ffigure{img/delta-encoding.png}{\de. The difference between the current and the previous value is stored instead of the actual value. Courtesy of \cite{Victor_Lavrenko2014-hv}.}{fig:delta-encoding}
\dele, or \term{Delta Compression}, is a form of compression where instead of storing the data itself, the difference between the previous and the current value is stored \cite{Wikipedia_contributors2015-cb}. This is illustrated in Figure \ref{fig:delta-encoding}. \de~is particularly effective i columns where the differences between consequtive values are small. Systems using this form for encoding include \vertica~\cite{Lamb2012-kg} and \blink~\cite{Raman2008-gi}. \dele~works particularly well on sorted data, since the sizes of the deltas are minimized. In addition, it handles columns with high cardinality better than other compression algorithms, like \rle~\cite{Stonebraker2005-qz}.

One of the major drawbacks of \dele~is that values cannot be accesed in constant time, that is to find a specific value at index $i$, the all the values from $0$ to $i - 1$ must be decoded. In addition, it is in general not trivial to work directly on the compressed data.

\subsection{\dele~and other compression techniques}
\label{sub:Delta Encoding and other compression techniques}
\bp~can be used in conjunction with \dele. If the deltas are small, which they typically will be if applied to a sorted column, they are candidates for bit packing. The column can be compressed even further by using \de, \dele, and \bp~a the same time, because the variance in the column values will be reduced when replaced with dictionary keys. However, applying \dele~to \de~results in that queries can no longer trivially work directly on the compressed data using simlpe integer operations. In other words, we get greater compression, but at the cost of reduction in performance.

An algorithm that combines \dele~with \bp~is \pfdelta, which is a bitpacked delta encoding \cite{Bjorklund2011-wh}. This method handles outliers very well, because they are put somewhere else in memory. It has the advantage of being able to be compressed and decompressed without branches. \todo{Elaborate on \pfdelta}

\ffigure{img/timestamp-compression.png}{Compression of timestamps in \gorilla. By \texttt{XOR}ing consecutive values, meaningful bits are extracted. Courtesy of \cite{Pelkonen2015-ko}.}{fig:timestamp-compression}
If the deltas are similar, like for instance if timestamp data where each data entry inserted into the database at a constant rate, another form of compression can be used. If the data is saved as doubles, a technique explained by Ratanaworabhan \ea~\cite{Ratanaworabhan2006-jb} can be used. This technique predicts each value and XOR it with the true value. If the predicted value is close to the actual value, only a few bits in the XOR result will be significant, and only the significant bits are saved. A similar compression algorithm is depicted in Figure \ref{fig:timestamp-compression}, an algorithm which is used by \gorilla~\cite{Pelkonen2015-ko}\todo{Drop this paragraph?}.

\section{Run-length encoding}
\label{sec:Run-length encoding}
\ffigure{img/rle.png}{\rle~on a sorted value range. Courtesy of \cite{Stoimen_undated-js}.}{fig:rle}
\rle~is a lossless data compression algorithm that replaces repeating data values with only one instance of the data and a number of how many times that value appear in the sequence \cite{Stoimen_undated-js}, like illustrated in Figure \ref{fig:rle}. Although it can be used for any sequence, \rle~works best on sorted data \cite{Bjorklund2011-wh, Holloway2008-rr}. In this case, each unique value in the sequence is represented exactly once, and this implies that \rle~works best for columns with low cardinality. \rle~is used in \cstore~\cite{Stonebraker2005-qz}, \vertica~\cite{Lamb2012-kg}, \oracle~\cite{Oracle2015-fs}, and \sapnw~\cite{Lemke2010-is}, but only if the values are sorted. \rle~can also be applied to compress sparse bitmaps \cite{Stonebraker2005-qz}.

Queries may benefit from \rle~on sorted columns when performing grouping and aggregation, since values are already put in groups. By keeping a prefix sum over all values, row ranges can easily be extracted for equality and range predicates. This is a form of letting the system work directly on the compressed data. \todo{Find source, rephrase}

%For sorted values, Run-length encoding can be used \cite{Bjorklund2011-wh}. A typical bla bla. The work by Holloway \ea~\cite{Holloway2008-rr} writes that run-length encoding works best on sorted data.

%Run-length encoding is used extensively in \cstore~and \vertica~\cite{Barber2012-xt} (which is the commercialization of \cstore). These databases allows for projections that are sorted, and will apply run-length encoding on columns with low cardinality that are sorted.

\section{Chapter Summary}
\label{sec:Chapter Summary}
We have in this chapter concluded that compression is required in a \bd~application. Not only does it reduce memory footprint, but it increases performance by reducing the number of CPU cycles, improve cache performance, and helps overcome the memory wall. Several database vendors claim compression is key for performance, including the top performing database in the \tpch, \exasol. To fully leverage the performance benefits of compression, compression must be light-weight and the queries must be processed directly on the compressed data.

With this is mind, we conclude that \de~in combination with \bp~is the most versitile and promising compression algorithm, and it does not require the columns to be sorted. We do however suggest using a sorted dictionary. One of the disadvantages with this approach is that it does not handle inserts and updates very well, but since we focus on a read-only system, this does not apply. If the values are sorted, and the column have low cardinality, \rle~should be considered, since this scheme also allows for executing queries directly on the compressed data. Applying \dele~can also be considered if the values are sorted, but only if reducing the memory footprint is more important than performance.


%QlikView uses a hash table to compress the data \cite{Qlik2014-vd}, and it yields a compression rate of 20\%-90\% dependent on the data.

%\oracle~utilizes a compression format on their in-memory storage layout \cite{Oracle2015-fs}. Compression is normally considered as a space-saving mechanism, but compression will also improve query performance. All scanning and filtering operations operate directly on the compressed columns which decreases query execution time. Data is decompressed before it is required for the result set. \oracle uses Dictionary Encoding, Run Length Encoding and Bit-Packing is used \cite{Oracle2015-fs}. 
