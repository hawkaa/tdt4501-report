\chapter{Design}

\section{Scale-up vs scale out}
Scaling out, that is using a cluster of server has long been considered the best way to design a high performance, high availability system for analytic workloads over large data sets \cite{Mukherjee2015-ul}. There are several reasons for this perception. First of all, the CPU and memory does not get contention as easily, since the data is spread across different servers in the cluster. With large data volumes, the main memory of a single instance might not be sufficient. Secondly, scaling out allows more servers to be added if the workload increases.

However, with decreasing harware prices, there has been a discussion wether scaling up is a better design choice. Analytic workloads from Yahoo and Microsoft has a median input size of 14 GB. Additionally, the implementation is easier because the entire server share the same address space. However, current state-of-the-art multi processors employ Non-uniform Memory Access (NUMA), such that a framework for distributed in-memory becomes necessary even on a single SMP server.

\section{Using a Colomn Store}
\label{sec:Using a Colomn Store}
Common for all the OLAP systems we have studied in this thesis, is that they all utilize a columnar storage layout. 

Additionaly, Abadi et al. \cite{Abadi2008-dd} supports this claim. Trying to emulate a column store in a row-store DBMS does not yield significant performance benefits.

\paragraph{Optimizations}
\label{par:Optimizations}
Using a raw column store will not automatically be advantageous over row-oriented databases. Abadi et al. \cite{Abadi2008-dd} demonstrate that simple column-oriented operations without optimizations, like compression and late materialization does not dramatically outperform well-designed row-stores.

\section{Read-only}
\label{sec:Read-only}
Adding support on-line updates clearly complicates the system design. We propose to keep the system read-only to reduce complexity. The in-memory data structures are loaded into memory in fixed intervals (e.g. every night). Omiting insert, update, and delete functionality will let us skip trasaction support, which is a tedious process and hard to get right.

This design choice does not completely disregard on-line updates in the future. We see that several clever solutions for transactional processing are present in today's products while still keeping the read optimized column store structure. The C-store uses a Writeable Store component to incremently add data to the Read-optimized Store. We believe a similar technique could be utilized in \projectName.

The fact that we choose to not support delta operations is based on the observation that there is two kind of analytics: \todo{Perhaps find background information on this?}
\begin{enumerate}
  \item \textbf{Immideate analycits} Analytics of something that just recently happened (e.g. today's sales). The data is required to be continiously updated, but the amount of data is low.
  \item \textbf{Historical analytics} Analytics that needs to compare large amounts of data (e.g. sales this year compared to previous year). Although handy, there is no requirement to have this data up to date all the time. Hence, there is a high amount of data, but it can be loaded into memory less frequently than the above.
\end{enumerate}


\section{Cherry-picking ideas}
\label{sec:Cherry-picking ideas}
We see that the different products have different ways of implementing high performance systems for query intensive workloads. The goal of this section is to pick the design principles to implement business intelligence in \genusSoftware.

\subsection{Data filters}
\label{sub:Data filters}
Both Oracle and QlikView offers some sort of data filtering for their analytics. As we saw in Section~\ref{sec:Oracle In-Memory option}, oracle lets the database administrators pick any database objects and add an \texttt{INMEMORY} option to it. More specifically, one can pick certain tables, certain columns and certain horizontal partitions (i.e records from the last year) to put in memory to reduce memory usage. Although QlikView is quite different, since the user must create a loading script, you still get the same functionality as Oracle when it comes to adding filters to your in-memory data.

For \genusSoftware, we introduce the concept of a Genus Data Mart. This lets the user define a snowflake schema with the required columns and tables for an analytical task. In addition to this, certain filters on the data should be applied, such that \projectName~supports horizontal partitioning of the data (i.e records from the last year). \todo{Figure out if the filters are needed}.

\subsection{Scalability}
\label{sub:Scalability}
Oracle offers support for highly distributed systems, such that high throughput and availability can be maintained.

Although this should be key design goal for all systems, we believe the extra complexity cannot be justified. \genus 's customers does not have more than ten servers.

We propose a scale-up architecture, where an entire data mart fits into the memory on a single server instance. However, different data marts will be placed on different servers, such that scale-out will be (partly) supported.

\subsection{Do not reinvent the OS}
\label{sub:Do not reinvent the OS}
We see that Oracle has implemented their own format for buffer management and raw files \todo{Check this out}.

On the other side, Monet embraces the idea of not re-inventing the operation system. It relies heavily on the virtual memory and the underlying file system. C-store does also use individual files in the underlying operating system to store the columns.

We consider utilizing the underlying operating system as a good design principle to keep \projectName~simple, yet effective.

\subsection{Compression}
\label{sub:Compression}
One of the major advantages by storing data by column, is that the data is intuitively more compressible than data stored in rows. Previous work has shown that query performance can be improved by an order of magnitude \cite{Abadi2006-bf}.

Originally, only seen as a way to reduce storage requirements, tests show that if a column-oriented query executor can operate on compressed data directly, performance can be improved \cite{Abadi2008-dd}

In order for compression to improve performance, it must satisfy two constraints \cite{Westmann2000-mz}. First, it must be very fast and have low CPU overhead. Otherwise, the performance benefits of reduced IO and better cache utilization will be eaten up by the additional CPU cost. Secondly, the compression must be fine grained. This is first of all for performance reasons, since fine-grained compression makes it possible to lazily decompress fields during query execution.

Dictionary-based compression is a common way of compressing the data in a column. There are several ways to go about this encoding. In order to maximize the compression performance, the number of distinct values must be known in advance, such that no more than the necessary bits will be used. We propose that all distinct values for every column should be counted before putting it in memory, such that the optimal number of bits for dictionary encoding may be used.

\subsection{Splitting columns horizontally}
\label{sub:Splitting columns horizontally}
We see that Microsoft and Oracle splits theirs column stores horizontally. SQL server operate on row groups which are groups of rows compressed into a columnar format. They say they must be small enough to benefit from in-memory operations and large enough to achive high compression rates. Oracle operates on In-Memory Compression Units, which is comprised of compressed columns known as a compression unit. Although no particular reasoning behind the IMCU size is found, it's likely to believe the arguments are the same as the ones for SQL server. Additionally, the Oracle Database In-memory supports distributed architechtures, where IMCU serves as the smallest distributed architecture. If the IMCUs are too large, load balancing might be suboptimal.

One additional argument for splitting the column is for insertions and updates. Managing columns in smaller partitions will improve performance on OLTP workloads

Since \projectName~is operates under the assumption that the entire database fits in memory, it should not support a distributed architecture, and the entire database is read-only, we find no reason to split columns horizontally. We propose a design where entire columns are stored contigously in memory.

\subsection{Vector Group By}
\label{sub:Vector Group By}
One should use the vector group by, since grouping happens regularly.

\section{Hardware assumptions}
In the design, we assume the following about the hardware.
\begin{enumerate}
  \item The entire dataset can be contained entirely in RAM.
  \item High performance 64-bits SMPs
\end{enumerate}

\section{Performance evaluation}
\label{sec:Performance evaluation}
The final design will be tested using the TPC-H benchmark. \todo{Add explanation about TPC-H benchmark}.

Another argument for scaling up instead of scaling out is that \genus has a detailed overview over their customers. They know that scaling up will suffice for all customers, and they can impose hardware requirements for their users.

\input{design/genus-data-mart}

\input{design/query-language}

\input{design/multitenancy-and-security}

\section{Why the already existing technologies are not used}
\label{sec:Why the already existing technologies are not used}
Genus uses the minimal of the database features in order to support a magnitude of SQL databases. Using In-memory constructs in these database is not a feasible option, due to two things:
\begin{itemize}
  \item Not all providers support in-memory options
  \item The syntax and configurations for these constructs are differet per provider.
\end{itemize}

\section{Future work}
\label{sec:Future work}
\subsection{Support for real time updates}
\label{sub:Support for real time updates}
In the first iteration of \projectName~development, there is no support for online updates. If such functionality should be provided, there are several techniques that can be used. We see that C-store \cite{Stonebraker2005-qz} is comprised of a writeable store (WS) and a Read-optimized store (RS), which are connected through a \textit{Tuple Mover}. A similar technique can be used for \projectName .
