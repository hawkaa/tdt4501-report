\chapter{Other Topics}
\label{chap:Other Topics}
In this chapter, we present topics and design considerations we have come across that do not fit in any of the other chapters. We start by explaining how application design and denormalization can affect performance. Then, we look into techniques of how to remove the in-memory and read-only assumptions.


\newpage

\section{Application Design}
\label{sec:Application Design}
In this report, we have looked into techniques of how to improve raw performance for an in-memory, read-only database. However, for a \bd~product, performance is considered from an end-user perspective \cite{Qlik2011-yc}. \qlikview~outlines four aspects of application performance: Size of data, the number of users, number of applications, and application design. The latter aspect means one should avoid too many listboxes, tables, and formulas. In addition to this, \bd~applications should keep columns split (e.g. \textit{first\_name}, \textit{last\_name} instead of \textit{name}) for improved dictionary compression, and a data extract should only include fields that are needed for the analysis.


In other words, not only designing a high-performance in-memory backend is important for the user experience; \textbf{much can be done in the application design}. The reporting panels should not be too complex, and data can be filtered or summarized during import. An example of this is to summarize transactions for a retail store in the data import script. By summing up the transaction amounts per store per day (or hour), data sizes are reduced, and the \bd~application performs better.

We have spent the entire report looking at techniques that can improve raw database and \bd~performance, but if data sizes can be reduced by one order of magnitude and still fulfill the analytical needs of the user, the \bd~application is likely to be one order of magnitude faster as well.


\section{Denormalization}
\label{sec:Denormalization}
 \term{Denormalization} is the technique where redundant data is added to a table in a database attempting to optimize performance \cite{Raman2008-gi, Wikipedia_contributors2015-az}. Among the read-optimized databases we have identified in this research, denormalization is not very common. First of all, denormalizing offsets the compression \cite{Barber2012-xt}. Second, applying a predicate to keys in a foreign table is easier and less time consuming than applying it on a denormalized fact table \cite{Abadi2008-dd}. 

\blink~was originally designed to operate on one, large denormalized table \cite{Johnson2008-cp}. However, after testing the database on real customer data, the authors figured out it did not help the database performance \cite{Barber2012-xt}. Therefore, a \textit{nested loop} based join was implemented instead. The authors never elaborated on why joining gave better performance than denormalization.

In the case of \qlikview, data is not denormalized within the in-memory database engine. However, as we have seen in Section \ref{sub:Data Import}, the data import step allows for denormalization to lower the number of tables in the application and reduce application complexity. We are, however, uncertain of the performance impact of denormalizing data in the data import step. 

\section{Disk Support}
\label{sec:Disk Support}

\afigure{img/page-thrashing.png}{Database performance for systems with and without a buffer manager. This figure shows two things. First, when a database system starts spilling pages to disk, the throughput is drastically reduced. Second, databases without buffer managers perform better when data is in memory, but when the OS starts swapping pages to disk, they are much slower than systems with a buffer manager. Courtesy of \cite{Graefe2014-ds}.}{fig:page-thrashing}{0.8}

In this report, we have assumed that the entire dataset fits in main memory for simplicity and performance reasons. Here, we elaborate on the consequences of relaxing this requirement. 

To support datasets larger than provisioned RAM, the most trivial way is to use the OS' virtual memory. This is done by \monetdb~\cite{Boncz2002-yj}, \blink~\cite{Barber2014-ey} and \qlikview~\cite{Qlik2011-ef}. However, the page replacement algorithms for virtual memory do not work very well on database workloads. The work of Graefe \ea~\cite{Graefe2014-ds} shows that main-memory databases suffers a sudden performance drop when virtual memory mechanism starts swapping pages to disk, as seen in Figure \ref{fig:page-thrashing}. \qlikview~has reported a significant loss in performance once the OS starts paging. 

Falling back to virtual memory are for some systems unacceptable for performance and correctness reasons \cite{Graefe2014-ds}. To overcome these challenges, major database vendors like \oracle~and \mssql~use buffer managers for all their database operations, which we see in Figure \ref{fig:page-thrashing} improves query performance when the working set is larger than the provisioned RAM. However, the same figure also reveals a drawback of using a buffer manager; the extra layer of indirection comes at a performance cost for working sets that fit in memory. The column store engine in \mssql~performs worse than the in-memory \vertipaq~on workloads that fit in memory because the latter does not have a buffer manager \cite{Ferrari2012-hm}

\tableau~claims their system still performs well even if the data does not fit entirely in memory \cite{Kamkolkar2015-iq}. We are not certain whether this system uses a buffer manager or not.


\section{Write Support and Mixed Workloads}
\label{sec:Write Support and Mixed Workloads}

We have focused on read-only systems in our research for simplicity and performance reasons. In this section, we look into how writes and mixed workloads can be supported.


We have studied several different systems in this report, and many of them have different goals. Some systems, like \cstore~and \monetdb, are optimized for analytical workloads even though they allow for inserts, updates, and deletes \cite{Boncz2002-yj, Stonebraker2005-qz}. Other systems have a design goal to support both transactional and analytical workloads (mixed workloads) equally well. Examples of such systems are \hyrise~and \hyper, and can be referred to as OLXP systems \cite{Plattner2014-fr}. \oracle~and \saph~also support mixed workloads, and they both strive to keep query transparency; applications using the database should not need to rewrite any queries to benefit from the underlying optimizations \cite{Farber2012-vh, Lahiri2015-mz}.

In Section \ref{sub:Read-Only}, we differentiated between \textit{update based write support} and \textit{direct write support}. In \textit{update based write support}, consistency, correctness, and data freshness can be sacrificed at the benefit of better performance; the system can use immutable structures optimized for read throughput. In such system, updates can periodically be merged into the database by for instance replacing one immutable structure with a more up-to-date version. If columns are partitioned horizontally, a column block can be added one at a time.

\hyper~sacrifices correctness and data freshness by using memory snapshots \cite{Kemper2011-ap}. In this system, there are multiple read-only processes, but only one process for writes. The read-only processes periodically fork the main write process to obtain a memory snapshot of the current state of the database. Hardware and OS assisted replication mechanisms make sure the snapshots are created efficiently and consistent with the transactional data.

\subsection{Delta Store}
\label{sub:Delta Store}

For systems with \textit{direct write support}, a \textit{delta store} is commonly used to handle inserts, updates, and deletes \cite{Raman2013-em, Stonebraker2005-qz}. This structure is sometimes called an insert buffer or a write-optimized store. A delta store is used because structures optimized for read performance are typically immutable or hard to update. For instance, inserting a new key into a sorted dictionary requires most key/value pairs to be reassigned. Besides, a newly inserted value might cause overflow in a bitpacked column.

\ffigure{img/delta-store-index}{Different index implementation for read-optimized store and delta store in \hyrise. The main partition uses an immutable structure for inverted indexes while the delta partition uses a dynamic tree structure. Courtesy of \cite{Schwalb2014-hn}.}{fig:delta-store-index}

Another immutable structure we have seen in this report is the inverted index structure using a position vector and an offset vector (prefix sum). This structure is effective and space efficient, but cannot be changed once created. This is why \hyrise~uses a tree-based dynamic index structure for the delta store, as seen in Figure \ref{fig:delta-store-index}.


Delta stores are periodically merged into the main storage, which is often called the read-optimized store. Such operation can either be triggered when the size of the delta store exceeds a certain threshold or through a periodical trickle operation \cite{Lahiri2015-mz, Farber2012-vh}.

In certain conditions, the delta store can be omitted completely on insertion. \mssql~allows bulk inserts to create columns directly without going via the delta store \cite{Larson2013-mc}.

Many systems disallow updates and instead replace an update operation with one deletion and one insert. Deletes are usually implemented as invalidation bit vectors \cite{Lamb2012-kg, Raman2013-em}, and updates are appended to a suitable structure, like an unsorted B+-tree \cite{Psaroudakis2014-ma}, or an uncompressed column \cite{Farber2012-vh}. \textit{Delete-insert} updates has the benefit of allowing time-travel queries \cite{Plattner2014-fr, Schwalb2014-hn}.

\section{Chapter Conclusion}
\label{sec:Chapter Conclusion}
In this chapter, we have seen that there are two mechanisms available if we need disk support. First, one can use the OS' virtual memory. This technique is efficient as long as the database fits in RAM but performs poorly once the OS starts swapping pages to disk. Second, we can use a buffer manager for better disk performance, but this comes with a performance penalty for datasets that fits within main memory.

We have also seen how writes and mixed workloads can be supported. For \textit{update based write support} systems periodical replacements of read-optimized structure or snapshots can be used. \textit{Direct write support} systems may benefit from a \textit{delta store} and \textit{delete-insert} updates.

We do not recommend denormalizing tables. Predicates on foreign keys are easier when the tables are separate, and denormalizing hurts compression.

We conclude this chapter by emphasizing the importance of proper application design. If we reduce the data size in a \bd~application and remove extraneous fields and GUI elements, the system is likely to perform better.
